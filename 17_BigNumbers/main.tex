Archimedes wrote the \emph{Sand Reckoner} to estimate how many grains of sand could fit in the cosmos. His method had nothing to do with measuring sand. The point was notation. He showed that even numbers larger than $10^{63}$ — far beyond everyday counting — could be handled systematically by grouping units into “orders” and assigning names to powers of powers. This was the first recorded move from numbers as quantities to numbers as formal objects.

Children count to ten using fingers. With time, they learn to write numbers with digits, to name hundreds, thousands, and eventually millions. The jump from ten to a million is steep, but manageable. Most people have never counted past a few thousand, yet they can write “one billion” or $10^9$ without hesitation. Scientific notation allows this: a base and an exponent replace long digit strings.

The number $10^{10}$ — ten billion — is roughly the number of humans who have ever lived. It is a countable quantity, close to the edge of what the brain can grasp without metaphor. The number of atoms in the observable universe is around $10^{80}$, much larger, but still tractable when written as “a 1 followed by 80 zeros.” These are numbers that no one counts, but everyone can represent.

Now consider $10^{10^{10}}$. This number has ten billion digits. Writing it out in base 10 would require more matter than exists in the universe. If each atom in the cosmos stored a digit, the task would fail. Yet the number is unambiguous. Its definition is compact. Its magnitude is fixed. It exists within mathematics as surely as “5” or “1000.”

Begin with 10. A child counts on fingers. Now $10^{10}$ — ten billion, roughly the number of humans who have ever lived. The physical universe contains approximately $10^{80}$ atoms. These numbers, though vast, remain comprehensible through scientific notation.

Consider $10^{10^{10}}$. This has ten billion digits. If each digit were an atom, we would need one ten-thousandth of the universe's atoms just to write it down. Yet this represents a tiny corner of mathematics. The number describes, perhaps, the probability of a specific quantum configuration arising by chance — every particle in the observable universe spontaneously arranging into a predetermined pattern.

Now build a tower: $10^{10^{10^{\cdot^{\cdot^{\cdot}}}}}$ with $10^{10}$ tens. To grasp this scale, imagine beings who create universes. They initiate Big Bangs with precise initial conditions, attempting to arrange cosmic evolution so that, 13.8 billion years later, a all 100 trillion green peas in the world land in a specific bucket on Earth's moon at exactly 8:00:00.00000... PM on Friday, July 4th, in a particular year. The initial conditions must account for every quantum fluctuation, every gravitational interaction, the formation of galaxies, stars, planets, the evolution of life, the development of agriculture, space travel, and someone deciding to throw that specific pea at that exact moment. 

The probability of success in one attempt is incomprehensibly small. Most attempts fail because Earth never forms, or forms differently, or life evolves differently, or humans never reach the moon, or a pea thrower is born a second too late. When an attempt fails, they can only start again — new Big Bang, new universe. Now every time they succeed, they get a virtual atom at random from one green pea to be at their score board. So after a while one pea will be concqoured virtually. And then another. And then another. And so on. If all the peas' atoms were concquired in order of their closeness to the moon's core, they can mark success and have breakfast. Otherwise, they can only start again. How long would it take until they will have breakfast?

Even if they initiate a new universe every attosecond ($10^{-18}$ seconds), running attempts for the entire lifetime of our universe ($10^{18}$ seconds), they achieve only $10^{36}$ trials. To expect even one success, they might need to repeat this process — universe-lifetime of attempts — some $10^{10^{10}}$ times. That approaches our tower of tens.

Exponent towers grow faster than any physical quantity. Let
\[
\begin{aligned}
T_1 &= 10, \\
T_2 &= 10^{10}, \\
T_3 &= 10^{10^{10}}, \\
T_4 &= 10^{10^{10^{10}}}, \\
&\ \vdots
\end{aligned}
\]
and so on, where each $T_k$ is a stack of $k$ tens. Even $T_5$ is too large to express using any notation with fixed base and exponent rules. This kind of growth overwhelms anything connected to physical processes.

The Ackermann function, often used to benchmark fast growth, escalates even faster. It is defined recursively by
\[
\begin{aligned}
A(0,n) &= n+1, \\
A(m+1,0) &= A(m,1), \\
A(m+1,n+1) &= A(m, A(m+1,n)).
\end{aligned}
\]
Its values increase modestly at first: $A(1,n) = n+2$, $A(2,n) = 2n+3$, $A(3,n) = 2^{n+3} - 3$. But $A(4,2)$ already exceeds a tower of 2’s with height $65,\!536$. By $A(5,1)$, the output exceeds all elementary notation. The function grows faster than any process built from loops with fixed bounds. It is often the first example of a total function that cannot be captured by “primitive recursion.”

To clarify the magnitude of these numbers, invert the scenario. Imagine a being capable of initiating universes. Their goal is to fine-tune the initial conditions of each universe so that, 13.8 billion years later, all green peas on Earth — around $10^{14}$ of them — land in a particular bucket on the Moon at exactly 8:00:00.000000 PM on July 4th of a chosen year.

The requirements include the formation of galaxies, stars, the Earth and Moon, the emergence of life, human civilization, agriculture, space travel, and the decision to launch a pea toward the Moon at exactly the right moment. Any deviation — Earth does not form, the Moon’s orbit is unstable, the individual is never born, or the timing is off by a microsecond — leads to failure.

Suppose one universe can be launched every attosecond ($10^{-18}$ seconds), and each universe runs for $10^{18}$ seconds — roughly the age of our own. That yields $10^{36}$ trials. Even $10^{10^{10}}$ such attempts would not guarantee success. The number of universes needed to expect this event may exceed $T_5$ or more. This kind of thought experiment does not illustrate probability. It illustrates failure of physical representation.

Now frame the question differently. Suppose the game is: write the largest number possible using exactly 20 centimeters of ink. The rules are strict. Every symbol must be defined in standard mathematical language. Every construction must be unambiguous. No appeals to "and so on" or informal recursion. The number that evaluates to the largest finite value wins.

Writing “$10^{10}$” takes fewer symbols than “$10000000000$,” so the former is more efficient. Adding exponents — $10^{10^{10}}$ — increases the magnitude without increasing length by much. Notation outpaces decimal form. Eventually, Knuth’s up-arrow notation becomes optimal. One arrow means exponentiation:
\[
3 \uparrow 3 = 3^3 = 27.
\]
Two arrows mean a tower: 
\[
3 \uparrow\uparrow 3 = 3^{3^3} = 3^{27}.
\]
Three arrows mean a tower whose height is itself a tower. With each added arrow, the expressive power increases dramatically.

Soon, defining the number matters more than naming it. A well-chosen function reference — like “$f_{64}(3)$” for a recursive sequence of arrow stacks — beats a billion digits of explicit output. The best use of ink is no longer to write digits, but to specify a method of generating a number. This is where numbers stop being quantities and become programs.

In the 20 cm ink game, most entries are not made of numbers. They are made of operations, encodings, references, and rules. The goal is not to evaluate — it is to define. The largest number is no longer the one with the most zeros. It is the one whose definition causes the longest known explosion of meaning.


Some of the largest finite numbers in mathematics arise not from tower functions or recursive formulas, but from simple combinatorial games. One of the most striking examples is the function $\mathrm{TREE}(n)$, defined using finite rooted trees with colored nodes.

Each node is labeled with a color from the set $\{1, 2, \dots, n\}$. The trees are finite, rooted, and unordered: the order of branches does not matter. The goal is to construct a sequence of such trees $T_1, T_2, \dots, T_k$, subject to two constraints:

1. The $i$-th tree must have at most $i$ nodes.

2. No earlier tree may be found within any later one as a matching pattern. More precisely: a tree $A$ is considered to appear in tree $B$ if there exists a subset of nodes in $B$ whose colors and parent-child relationships exactly mirror those of $A$.

This rule prohibits repetition. A later tree cannot contain any configuration already seen in an earlier one, even in a small corner of its structure.

The function $\mathrm{TREE}(n)$ is defined as the maximal length of such a sequence.

With one color, only one tree can be used: any second tree will contain the first. So $\mathrm{TREE}(1) = 1$. With two colors, one can write three trees before the containment rule becomes unavoidable. So $\mathrm{TREE}(2) = 3$.

For $\mathrm{TREE}(3)$, the number is finite but incomprehensibly large. It exceeds $10^{10^{10^{10}}}$, $f_k(n)$ for any primitive-recursive tower function $f_k$, and even Graham’s number. It is larger than any number constructible in Peano Arithmetic. Proving that $\mathrm{TREE}(3)$ is finite already requires second-order arithmetic and deep theorems about well-quasi-orderings. 

There exists a specific sequence of trees of length $\mathrm{TREE}(3)$ that satisfies all the rules. But the number of steps it allows is so large that it defies any representation, compression, or approximation. It is not infinite. It is a concrete, finite number, with a fixed decimal expansion. But its definition — not its size — grants it this status.

Another example of an enormous but well-defined number comes from computation theory. The busy beaver function $\mathrm{BB}(n)$ is defined as follows: among all Turing machines with $n$ internal states that eventually halt when started on a blank tape, $\mathrm{BB}(n)$ is the maximum number of steps any such machine takes before halting.

The first few values are known: $\mathrm{BB}(1) = 1$, $\mathrm{BB}(2) = 6$, $\mathrm{BB}(3) = 21$, $\mathrm{BB}(4) = 107$. But by $n = 5$ or $n = 6$, the values become unknown. Unlike $\mathrm{TREE}(3)$, the growth of $\mathrm{BB}(n)$ is not merely combinatorial. It arises from undecidability. The function cannot be computed: no algorithm can determine $\mathrm{BB}(n)$ for arbitrary $n$, because doing so would solve the halting problem.

Yet for each $n$, $\mathrm{BB}(n)$ is a specific natural number. It is not a symbol for growth. It is a value — one that could, in principle, be written out in decimal form, if only it could be found. It may even be smaller than $\mathrm{TREE}(3)$ at some points. But the obstruction is not size — it is logical impossibility. There is no mechanical way to distinguish halting machines from non-halting ones. The function grows faster than any computable function, not because of speed, but because of epistemological failure.

A third class of large numbers arises from definability. Rayo's function, written $\mathrm{Rayo}(n)$, is defined as the largest natural number that can be specified using at most $n$ symbols in the language of first-order set theory. The language includes logical quantifiers, variables, membership, equality, and a finite set of symbols chosen in advance. Each formula written in this language denotes a unique natural number. Among all such formulas of length at most $n$, $\mathrm{Rayo}(n)$ is the maximum output.

This function expands the idea of the ink game. Instead of writing digits, one writes a definition. The longer the definition, the larger the number that can be specified. The symbol budget is fixed. The space of possible definitions expands with each additional token. Each time the notation becomes more expressive, new numbers can be reached.

The function is carefully designed to avoid self-reference and paradox. For instance, the phrase “the largest number definable in fewer than $n$ symbols” must not itself fit within $n$ symbols. Formal constraints are added to make sure the definitions remain well-founded.

$\mathrm{Rayo}(n)$ grows faster than any explicitly definable function in the same language. It does not arise from trees or machines. It arises from the space of meanings that fit within bounded description.

What distinguishes these three examples — $\mathrm{TREE}(n)$, $\mathrm{BB}(n)$, $\mathrm{Rayo}(n)$ — is the source of their magnitude:

- $\mathrm{TREE}(3)$ is large because its construction avoids structural(???) repetition.
- $\mathrm{BB}(n)$ is large because halting behavior is undecidable.
- $\mathrm{Rayo}(n)$ is large because the space of definitions grows faster than any bounded enumeration.

Each number is finite. Each one exists. But none of them can be calculated, approximated, or related to any process in the physical universe.

They occupy different positions in the hierarchy of inaccessibility. $\mathrm{TREE}(3)$ can be bounded using tools from set theory and combinatorics. $\mathrm{BB}(n)$ resists analysis by any algorithm. $\mathrm{Rayo}(n)$ resists analysis by any system within its own language. The functions are not merely fast-growing. They reflect distinct limits: structural, computational, and definitional.

These are not large numbers in the usual sense. They are precise finite quantities that reveal what finite mathematics is capable of expressing — and what it cannot reach. (WTF???)





The number of atoms in the observable universe is massive, obviously incomprehensible. It can be written as 1 followed by 80 zeros, or $10^{80}$. But even some kids know numbers that dwarf that.

A googol, for example, is $10^{100}$—that's 1 followed by 100 zeros. A googolplex is $10^{\text{googol}}$, or $10^{10^{100}}$. But this is basically $10^{(10^{100})}$, which is pretty compact to write.

One can write even bigger numbers. How about $10^{10^{\cdots^{10}}}$ with ten 10s in the tower? Or a hundred 10s? The pattern is clear—we're building towers of exponents, each level making the number incomprehensibly larger.

It turns out that for some problems in combinatorics, even bigger numbers are needed. This is where we meet Graham's number. To understand it, we need Knuth's up-arrow notation, which is a clever way to write very large numbers compactly.

Start simple: $3 \uparrow 3$ just means $3^3 = 27$. Nothing special yet. But $3 \uparrow\uparrow 3$ means $3^{3^3} = 3^{27}$, which is already over 7 trillion. And $3 \uparrow\uparrow\uparrow 3$ means we make a tower of 3s that's 27 levels high: $3^{3^{3^{\cdots}}}$ with 27 threes total.

Each arrow represents a completely different level of growth. One arrow is exponentiation. Two arrows make towers. Three arrows make towers whose height is itself a tower. Four arrows... well, it gets hard to even describe in words.

Graham's number starts with $3 \uparrow\uparrow\uparrow\uparrow 3$ and then does this operation 64 times, each time increasing the number of arrows. It's finite, explicitly defined, and yet so large that even describing how many digits it has is impossible.

The definition can be either a specific number, or the growth of a function. For instance, $f(n) = 10^n$ grows fast. $f(n) = 10^{10^{\cdots^{10}}}$ with $n$ tens grows faster. Some definitions can be recursive: $f_k(n) = 10^{10^{\cdots^{10}}}$ where the tower has $f_{k-1}(n)$ levels. You can imagine how fast this can grow.

This connects to the Ackermann function, which was designed to grow faster than any ``primitive recursive'' function. Primitive recursive functions are built from basic operations like addition and multiplication using only simple loops—no fancy recursion. The Ackermann function breaks this limitation.

Here's how it works: $A(0,n) = n+1$, $A(1,n) = n+2$, $A(2,n) = 2n+3$, $A(3,n) = 2^{n+3} - 3$. Each level uses the previous level recursively. By the time we reach $A(4,2)$, we get a tower of 2s that's 65,536 levels high. $A(5,n)$ grows so fast that no simple description suffices.

One insanely fast growing function, where even $f(3)$ is massive, is the TREE function. Let me explain the game behind it.

Imagine you're drawing trees—not forest trees, but mathematical trees made of dots connected by lines. Each dot gets a label: 1, 2, or 3. The rule is simple: draw a sequence of labeled trees, but no tree in your sequence can be a ``subtree'' of any tree that comes later.

What's a subtree? If you can delete some dots and lines from a bigger tree and get a smaller tree (keeping all the labels), then the smaller one is a subtree of the bigger one.

TREE(1) asks: how long can your sequence be if you only use label 1? The answer is 1—you can only draw one tree (a single dot labeled 1) because any second tree would have to contain this first tree as a subtree.

TREE(2) = 3. You can draw three trees before being forced to stop. The three trees counted by TREE(2) are the only rooted trees with nodes labeled 1 or 2 that do not contain each other as subtrees (under a specific containment definition). These trees are: A single node labeled 1, A single node labeled 2, A root labeled 2 with one child labeled 1.

Once you've used those three, any new tree you build using labels $\{1, 2\}$ will necessarily contain one of those three as a subtree, which violates the TREE sequence condition.

TREE$(n)$ is defined as the length of the longest sequence of rooted trees using labels $\{1,\ldots,n\}$ where no tree is a (topological) subtree of a later one.

TREE(3) is where things get crazy.

To grasp TREE(3)'s magnitude, consider this comparison: if every atom in the observable universe became a googolplex ($10^{10^{100}}$), and these googolplex-atoms multiplied together every nanosecond throughout cosmic history — 13.8 billion years of continuous multiplication — the result wouldn't reach one trillionth of TREE(3).

Let's make it a game, informal. You have a pen with enough ink to write a 20cm line. The one who writes the biggest number wins. One rule: use any symbol that can be unambiguously understood by a group of 10 mathematicians.

Here emerges a divide about accessibility. Some big numbers are easy to understand, like TREE(3) — it comes from a simple game that children can play and you know it has a specific sequence of digits, and it may even be possible to know what some of them are. Others are a bit accessible, like the busy beaver function, which requires understanding computers and algorithms. Still others are close to philosophical, like Rayo's function, which ventures into logic and language.

The busy beaver function $BB(n)$ asks: what's the maximum number of steps that any $n$-state computer program can run before stopping? This grows faster than any computable function. $BB(100)$ exists but can never be calculated by any computer, no matter how powerful.

Rayo's function is more abstract. Rayo$(n)$ is defined as the largest number expressible in first-order set theory using at most $n$ symbols. This blends mathematics with language and pushes against the boundaries of what can even be defined. For example, if I have Rayo$(n)$ as the largest number I can write with $n$ symbols, what prevents me to write Rayo$(n)+1$ with less than $n$ symbols (for $n>10$ or so)? So it is less solid than something like TREE(3).

But here's the thing: while TREE(3) is so massive, it's not scratching the distance to infinity, even the smallest infinity.

Let me explain cardinality quickly. Two sets have the same ``size'' (cardinality) if you can match up their elements one-to-one. The integers $\{1, 2, 3, \ldots\}$ and the even numbers $\{2, 4, 6, \ldots\}$ have the same cardinality because you can pair them: $1 \leftrightarrow 2$, $2 \leftrightarrow 4$, $3 \leftrightarrow 6$, and so on.

The smallest infinity is $\aleph_0$ (aleph-null)—the cardinality of the natural numbers. Even though the rational numbers seem ``denser'' than integers, they're still countable and have cardinality $\aleph_0$.

TREE(TREE($\ldots$TREE(3)$\ldots$)) done TREE(3) times is as far from infinity as 8. All finite numbers, no matter how large, are equally distant from infinity.

The real numbers have cardinality $\mathfrak{c}$ (continuum), which is strictly larger than $\aleph_0$. Cantor's diagonal argument proves you can't match up natural numbers with real numbers one-to-one.

While the smallest infinity is so big, there are cardinalities that are incomprehensible. We now briefly list some of the largest named infinities, each one reaching further beyond:

\begin{itemize}
    \item $\omega$ — The first infinite ordinal, representing the order type of the natural numbers. It marks the transition from finite counting to a structured form of infinity. Every natural number is less than $\omega$, but $\omega$ itself has no predecessor.
    
    \item $\omega^\omega$ — A higher countable ordinal representing an infinite hierarchy of increasing exponents: $\omega, \omega^\omega, \omega^{\omega^\omega}, \ldots$ It captures the idea of infinitely layered growth within the realm of countable infinities. Though still countable, its structure is already beyond what most recursive processes can describe easily.
    
    \item $\varepsilon_0$ — The first solution to the equation $\omega^\alpha = \alpha$. It is the limit of the sequence $\omega, \omega^\omega, \omega^{\omega^\omega}, \ldots$, continuing infinitely. This ordinal marks the boundary of what Peano Arithmetic can prove using transfinite induction.
    
    \item $\varphi_\alpha(\beta)$ — The Veblen hierarchy extends ordinal exponentiation by defining fixed points of fixed points, and so on. For example, $\varphi_0(\beta) = \omega^\beta$, but $\varphi_1(0)$ is already $\varepsilon_0$, and the function $\varphi_\alpha$ defines ever-larger ordinals through transfinite recursion. This framework captures an explosion of size that eventually surpasses anything built with exponentials alone.
    
    \item $\Gamma_0$ — The smallest ordinal that cannot be reached using the Veblen hierarchy with finitely many steps and previously defined operations. It is the proof-theoretic ordinal of systems like ATR$_0$ (arithmetical transfinite recursion), meaning it marks the point where classical mathematics ends and impredicative methods begin. Any reasoning involving ordinals beyond $\Gamma_0$ requires stepping outside predicative arithmetic.
    
    \item $\omega_1^{CK}$ — The Church–Kleene ordinal is the smallest ordinal not describable by any computable process. It is the limit of all ordinal notations that can be encoded and manipulated by a Turing machine. Though still countable, it represents the boundary of algorithmic mathematics.
    
    \item $\omega_1$ — The first uncountable ordinal, containing all countable ordinals as elements. It is the smallest ordinal with uncountable cardinality and cannot be listed in any sequence indexed by the natural numbers. It marks the threshold between the countable and the genuinely uncountable.
    
    \item Inaccessible Cardinals — These are uncountable cardinals so large that they cannot be reached by standard set-theoretic operations like taking powersets or unions. They are regular (uncountable and not the limit of smaller cardinals) and strong limits (larger than any power set of a smaller cardinal). Their existence cannot be proven within ZFC and signals the start of the large cardinal hierarchy.
    
    \item Measurable Cardinals — Cardinals $\kappa$ that carry a nontrivial, $\kappa$-complete ultrafilter: a way of assigning ``measure one'' to certain subsets of $\kappa$ that respects large intersections. This allows for a probability-like measure over sets of size $\kappa$, an idea that cannot be realized at smaller scales. Measurable cardinals enable results about the structure of the set-theoretic universe, including elementary embeddings.
    
    \item Supercompact Cardinals — These are stronger than measurable cardinals and can reflect the structure of the universe at any larger size $\lambda$ down to themselves. If $\kappa$ is supercompact, then any structure of size $\lambda$ can be ``seen'' inside a smaller structure of size $\kappa$ in a way that preserves logical truths. Their existence implies an immense richness in the set-theoretic universe.
\end{itemize}

We can also remember that each huge number $N$ has a very small number $1/N$ that is equivalently bogglingly small!

The inverse of numbers is a mental zoo! Human intuition, evolved for quantities measured in dozens or hundreds, encounters systematic failure when confronting mathematical vastness. One million seconds equals roughly 11.5 days; one billion seconds spans 31.7 years. Yet even this thousand-fold difference becomes meaningless before TREE(3).

The game of large ordinals passes from mathematics into awe. I do not claim to grasp it the way someone like Shelah does—no one rivals him in depth or vision in transfinite combinatorics. But this chapter, uniquely in the book, breaks from accessibility. It is not here to explain, but to evoke.

Oswald Veblen (1880–1960) proved the Jordan curve theorem in 1905—an interesting problem asking if a non-self-intersecting curve on a plane divides it into two regions. The answer is yes, but the proof is not trivial when the curve lacks simple geometry.

Large numbers expose what humans consider ``large.'' Our planet contains roughly $10^{50}$ atoms—vast until compared to TREE(3). The observable universe, with approximately $10^{80}$ atoms, represents no meaningful progress toward mathematical infinity.

Big numbers live in a precise intermediate space. They are finitely describable, but so large as to be practically unreachable. In contemplating these numerical extremes, we encounter the boundary between finite mathematical objects and the infinite mathematical universe beyond. The mental zoo of large numbers reveals not just quantitative extremes, but qualitative breaks in the very nature of mathematical existence.




For Big Numbers, we try to commicate how massive is TREE(3). So we need to tell a story about for example 10 adn then 10^10 and then about 10^10^10 (eg histories of current universe) and then 10^10^10^...^10 10^10 times (eg, someone make a big bang to get that exaclty one green pea will arrive in a specific buckt on the moon because of some weird conicidecs exactly at 8:00:00:00 on Friday the 4th etc. And if it fails (due to not created earth or any other reason) they can only start big bang again. 

after that number above, it will happen trillions of times... that is a massiv number. yet even doing 10^10 each attosecond while universe exist, not close to TREE(3)