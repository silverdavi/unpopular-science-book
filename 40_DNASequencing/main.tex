DNA carries genetic information in a linear sequence of four chemical bases: adenine (A), cytosine (C), guanine (G), and thymine (T). Reading this sequence — determining the exact order of bases in a DNA molecule — requires overcoming a fundamental scale mismatch. Individual bases measure roughly one nanometer, while human chromosomes stretch millions of bases long. No technology can directly read such lengths in one pass.

The solution involves fragmenting DNA into manageable pieces, reading each fragment separately, then computationally reconstructing the original sequence. This creates two distinct challenges: the biochemical problem of reading individual fragments and the algorithmic problem of assembling them correctly. Each generation of sequencing technology has approached these challenges differently.

Reading the linear sequence of DNA's four chemical bases—adenine (A), cytosine (C), guanine (G), and thymine (T)—requires overcoming a scale mismatch. Individual bases are nanometers wide, while human chromosomes are millions of bases long. The solution is to fragment DNA, read each piece separately, and computationally reconstruct the original sequence. This creates two challenges: biochemical reading of individual fragments and algorithmic assembly of the complete sequence.

Frederick Sanger solved the reading problem through controlled interruption of DNA synthesis. DNA polymerase builds new strands by adding nucleotides complementary to a template; a 3'-hydroxyl group on each nucleotide enables the next to attach. Sanger introduced dideoxynucleotides (ddNTPs) that lack this hydroxyl group, terminating synthesis when incorporated.

This process is probabilistic. In a mix containing a DNA template, primers, polymerase, all four normal dNTPs, and a small amount of one ddNTP type (e.g., ddATP), the polymerase occasionally incorporates a ddATP, terminating the strand. Across millions of template copies, this generates a collection of fragments of different lengths, each ending at a different A position.

Running four parallel reactions—one for each ddNTP type—produces four fragment collections. Gel electrophoresis separates these by size: DNA fragments migrate through a polymer matrix under an electric field, with smaller fragments moving faster. After separation, each gel lane shows a ladder of bands. Reading from shortest to longest fragment across all four lanes reveals the sequence. If the shortest fragment appears in the G lane, the first base is G. If the next shortest is in the A lane, the second base is A.

This method powered the Human Genome Project but had fundamental limitations. Each reaction produced only 500-1000 readable bases. Preparing samples, running gels, and reading results consumed hours per reaction. Radioactive or fluorescent labeling added complexity and cost. The throughput ceiling meant that sequencing a human genome required years of work and hundreds of millions of dollars.

Next-generation platforms achieved a breakthrough with parallelization, performing millions of reactions simultaneously on a single surface. Early systems distributed single DNA fragments into millions of microscopic wells and flowed one nucleotide type at a time (first all As, wash; then all Cs, wash). Detection chemistry varied. 454 Life Sciences used pyrosequencing: nucleotide incorporation releases pyrophosphate (PPi), which an enzyme cascade converts into a light signal via luciferase. Ion Torrent used semiconductor sequencing: incorporation releases a hydrogen ion, and millions of ISFET sensors detect the resulting pH change as a voltage signal. Both methods suffered from the same core limitation. Because nucleotides were added sequentially, homopolymer runs like AAAA caused all four bases to incorporate at once, producing a signal four times stronger. Distinguishing a 4× signal from a 5× signal was error-prone, limiting accuracy.

Illumina took a different path, solving the homopolymer problem through reversible termination. Their innovation combined three key elements: surface-bound amplification, chemically cleavable terminators, and four-color imaging.

The process begins with bridge amplification. DNA fragments attach to a glass surface coated with two types of oligonucleotide primers. Each fragment bends to hybridize with a nearby complementary primer, forming a bridge. Polymerase extends the primer, creating a complementary strand anchored at both ends. Denaturation releases the original strand, and the process repeats. After 35 cycles, each original molecule generates a tight cluster of ~1000 identical copies, all within a few hundred nanometers — small enough to act as a single sequencing unit but bright enough for fluorescence detection.

Illumina's sequencing chemistry uses nucleotides engineered with two modifications: a fluorescent dye unique to each base (A, C, G, T) and a chemical block on the 3'-OH that prevents further extension. Unlike Sanger's permanent terminators, these blocks can be cleaved chemically.

Each sequencing cycle follows four steps: add all four labeled terminators simultaneously, wait for incorporation, image in four colors, then cleave both dye and terminator. Because only one base can be added per cycle (due to the 3'-block), homopolymers read accurately — AAAA requires four separate cycles, each adding one A. This solved 454's fundamental limitation.

Illumina's paired-end innovation provided crucial structural information. Sequence both ends of a DNA fragment, keeping track that they came from the same molecule. If fragments are 500 bases long but you only read 150 bases from each end, you know those two 150-base sequences sit exactly 200 bases apart in the genome. These distance constraints prove essential for genome assembly.

Assembling a 3-billion-base human genome from 20 million 150-base fragments is computationally demanding. Early overlap-layout-consensus algorithms, which compare all read pairs to find overlaps, were feasible for thousands of Sanger reads but fail for millions of short reads where all-pairs comparison is prohibitive.

De Bruijn graphs, a 1946 mathematical structure, provided a scalable solution. Instead of connecting reads, they connect k-mers—all possible k-letter substrings. A sequence traces a path through a graph where each unique k-mer is a node and edges connect k-mers overlapping by k-1 bases. The scalability arises because a genome of length G contains at most G distinct k-mers, regardless of sequencing depth. Finding Eulerian paths that traverse each edge once is tractable even for graphs with billions of nodes. take the sequence ATCGATCG and extract all 3-mers: ATC, TCG, CGA, GAT, ATC, TCG. Build a graph where each unique k-mer is a node, and edges connect k-mers that overlap by k-1 bases. The sequence ATCGATCG traces a path through this graph: ATC→TCG→CGA→GAT→ATC→TCG.

Repeats in the genome, such as transposable elements, complicate assembly. When a repeat is longer than a read, it creates ambiguity in the assembly graph, resulting in multiple valid paths. Paired-end constraints resolve some ambiguities, but short reads cannot span long repeats, which required the development of long-read technologies.

Pacific Biosciences (PacBio) developed single-molecule real-time (SMRT) sequencing, observing individual DNA polymerase enzymes. The primary challenge was detecting single fluorescent nucleotides against the background of unincorporated ones. The solution was zero-mode waveguides (ZMWs): 70-nanometer holes in an aluminum film that confine laser illumination to a 20-zeptoliter volume. A polymerase at the bottom of each ZMW holds an incorporating nucleotide for milliseconds, long enough to generate a detectable fluorescent flash distinct from the transient signals of freely diffusing nucleotides. This method generates reads exceeding 10,000 bases. Though noisy, with error rates of 10-15\%, these long reads are effective at spanning genomic repeats.

Oxford Nanopore technology uses no enzymes or fluorescence. It passes a single DNA strand through a protein nanopore embedded in a membrane. An applied voltage drives both the DNA and an ionic current. As the DNA translocates, nucleotides in the pore's 1.4-nanometer constriction modulate the current. The narrowest region spans approximately five bases, so the signal reflects a 5-mer. Neural networks decode the complex current modulations into a DNA sequence, achieving >95\% accuracy and read lengths that can exceed one million bases.

Long reads simplified assembly graphs dramatically, as most repeats become trivial to span. Modern projects often use a hybrid approach: Illumina provides an accurate short-read backbone, while PacBio or Nanopore provides a long-read scaffold to resolve repeats and structural variants.
\newpage
\begin{commentary}[On Assembly Statistics]
Evaluating a genome assembly requires understanding its output format. Assemblies consist of fragments at two organizational levels. A \textbf{contig} is a contiguous stretch of sequence assembled from overlapping reads — an unbroken text. A \textbf{scaffold} links multiple contigs that are ordered and oriented but separated by gaps of estimated size. Consider recovering a book's text from shredded copies: contigs are complete pages reconstructed from overlapping fragments, scaffolds are chapters where page order is known but some pages remain missing.

Assembly statistics quantify output quality. The median contig length — the middle value in a sorted list — is uninformative because assemblies contain thousands of short contigs and few long ones. An assembly with 10,000 contigs might have a median length of 500 bases while its longest contigs exceed one million bases.

The \textbf{N50} statistic measures contiguity differently. Sort contigs from longest to shortest, then sum their lengths sequentially. The N50 is the length of the contig that brings this cumulative sum to 50\% of the total assembly size. It identifies the minimum length such that half the genome resides in contigs of that length or longer. For contigs of lengths 10, 9, 8, 7, 6, 5, 4, 3, and 2 kb (total 54 kb), the cumulative sum reaches 27 kb (50\%) after adding the first three contigs (10+9+8). The N50 is 8 kb — the length of that third contig. The median is 6 kb.

The scientific literature frequently misreports N50 as "median contig length (N50)." Analysis of PubMed publications identified thousands of papers containing this error (from sampling the word contig, I got within 95\% confidence interval around 12\% of the total!). The N50 is a length-weighted metric, not a median. Describing N50 as a "weighted median" is correct if one creates an expanded list where each contig appears once for each base it contains, then takes that list's median.

This type of error is common when multidisciplinary knowledge is needed across distant fields such as biology and information science.
\end{commentary}

